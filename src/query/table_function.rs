use std::ffi::{CStr, CString};
use std::os::raw::c_void;
use std::sync::atomic::{AtomicBool, Ordering};

use duckdb::{
    core::{DataChunkHandle, Inserter, LogicalTypeHandle, LogicalTypeId},
    vtab::{BindInfo, InitInfo, TableFunctionInfo, VTab, Value},
};
use libduckdb_sys as ffi;

use crate::catalog::CatalogState;
use crate::expand::{expand, suggest_closest, QueryRequest};
use crate::model::SemanticViewDefinition;

use super::error::QueryError;

// ---------------------------------------------------------------------------
// QueryState -- stored as extra_info on the table function
// ---------------------------------------------------------------------------

/// Shared state for the `semantic_query` table function.
///
/// Carries the catalog (in-memory view definitions) and a raw `duckdb_connection`
/// handle for executing expanded SQL via the C API. The connection is created
/// independently of the host connection by calling `duckdb_connect` on the same
/// database handle, avoiding lock conflicts with the host during query execution.
///
/// For the connection handle extraction strategy, see `extract_query_conn` in lib.rs.
#[derive(Clone)]
pub struct QueryState {
    pub catalog: CatalogState,
    /// Raw connection handle for SQL execution.
    pub conn: ffi::duckdb_connection,
}

// SAFETY: The `duckdb_connection` raw pointer is an opaque handle managed by DuckDB.
// It was created during `extension_entrypoint` and lives for the database session lifetime.
// DuckDB manages internal synchronization for connections.
unsafe impl Send for QueryState {}
unsafe impl Sync for QueryState {}

// ---------------------------------------------------------------------------
// BindData / InitData
// ---------------------------------------------------------------------------

/// Data computed at bind time: the expanded SQL, output schema, and metadata.
pub struct SemanticViewBindData {
    /// The SQL query generated by `expand()`, ready for execution.
    expanded_sql: String,
    /// Output column names (dimensions first, then metrics).
    column_names: Vec<String>,
    /// Output column types as DuckDB C API type enums (stored for error context).
    #[allow(dead_code)]
    column_type_ids: Vec<ffi::duckdb_type>,
}

// SAFETY: All fields are `Send + Sync` types.
unsafe impl Send for SemanticViewBindData {}
unsafe impl Sync for SemanticViewBindData {}

/// State tracked during function execution.
pub struct SemanticViewInitData {
    /// Signals when all rows have been emitted.
    done: AtomicBool,
}

// SAFETY: `AtomicBool` is `Send + Sync`.
unsafe impl Send for SemanticViewInitData {}
unsafe impl Sync for SemanticViewInitData {}

// ---------------------------------------------------------------------------
// FFI helpers
// ---------------------------------------------------------------------------

/// Execute a SQL string via the DuckDB C API and return the result.
///
/// The caller is responsible for calling `duckdb_destroy_result` on the returned
/// result when done.
///
/// # Safety
///
/// `conn` must be a valid, non-null `duckdb_connection` handle.
pub(crate) unsafe fn execute_sql_raw(
    conn: ffi::duckdb_connection,
    sql: &str,
) -> Result<ffi::duckdb_result, String> {
    let sql_cstr = CString::new(sql).map_err(|e| e.to_string())?;
    let mut result: ffi::duckdb_result = std::mem::zeroed();
    let rc = ffi::duckdb_query(conn, sql_cstr.as_ptr(), &mut result);
    if rc != ffi::DuckDBSuccess {
        let err_ptr = ffi::duckdb_result_error(&mut result);
        let err_msg = if err_ptr.is_null() {
            "unknown error".to_string()
        } else {
            CStr::from_ptr(err_ptr).to_string_lossy().into_owned()
        };
        ffi::duckdb_destroy_result(&mut result);
        return Err(err_msg);
    }
    Ok(result)
}

/// Create a `LogicalTypeHandle` from a DuckDB C API type enum.
///
/// Uses `duckdb_create_logical_type` for primitive types. Falls back to VARCHAR
/// for complex types (DECIMAL, LIST, STRUCT, etc.) since those require
/// additional parameters.
///
/// Currently unused -- all output columns are declared as VARCHAR. Retained
/// for potential future use if typed output columns are re-enabled.
#[allow(dead_code)]
fn logical_type_from_duckdb_type(ty: ffi::duckdb_type) -> LogicalTypeHandle {
    // For primitive types, use the duckdb-rs factory.
    // For complex/unsupported types, default to VARCHAR.
    match ty {
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_BOOLEAN => LogicalTypeHandle::from(LogicalTypeId::Boolean),
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_TINYINT => LogicalTypeHandle::from(LogicalTypeId::Tinyint),
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_SMALLINT => LogicalTypeHandle::from(LogicalTypeId::Smallint),
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_INTEGER => LogicalTypeHandle::from(LogicalTypeId::Integer),
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_BIGINT => LogicalTypeHandle::from(LogicalTypeId::Bigint),
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_UTINYINT => LogicalTypeHandle::from(LogicalTypeId::UTinyint),
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_USMALLINT => LogicalTypeHandle::from(LogicalTypeId::USmallint),
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_UINTEGER => LogicalTypeHandle::from(LogicalTypeId::UInteger),
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_UBIGINT => LogicalTypeHandle::from(LogicalTypeId::UBigint),
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_FLOAT => LogicalTypeHandle::from(LogicalTypeId::Float),
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_DOUBLE => LogicalTypeHandle::from(LogicalTypeId::Double),
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_TIMESTAMP => LogicalTypeHandle::from(LogicalTypeId::Timestamp),
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_DATE => LogicalTypeHandle::from(LogicalTypeId::Date),
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_TIME => LogicalTypeHandle::from(LogicalTypeId::Time),
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_VARCHAR => LogicalTypeHandle::from(LogicalTypeId::Varchar),
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_BLOB => LogicalTypeHandle::from(LogicalTypeId::Blob),
        ffi::DUCKDB_TYPE_DUCKDB_TYPE_HUGEINT => LogicalTypeHandle::from(LogicalTypeId::Hugeint),
        _ => LogicalTypeHandle::from(LogicalTypeId::Varchar), // Safe fallback
    }
}

/// Extract the raw `duckdb_value` pointer from a `duckdb::vtab::Value`.
///
/// The `Value` struct has a single field `ptr: duckdb_value` (which is `*mut c_void`),
/// but it is `pub(crate)` and inaccessible from external crates.  Since both types
/// are exactly one pointer wide, transmute is a safe layout conversion.
///
/// The returned pointer is valid only for the lifetime of the `Value` (it is NOT
/// destroyed; the `Value` still owns it).
///
/// # Safety
///
/// Relies on `duckdb::vtab::Value` layout being a single `duckdb_value` field.
/// This is stable for `duckdb = "=1.4.4"`.
unsafe fn value_raw_ptr(value: &Value) -> ffi::duckdb_value {
    // Value is repr(Rust) with one field: duckdb_value (*mut c_void).
    // Reading the pointer without taking ownership.
    let ptr_to_value = std::ptr::from_ref(value).cast::<ffi::duckdb_value>();
    std::ptr::read(ptr_to_value)
}

/// Extract string elements from a DuckDB LIST(VARCHAR) value.
///
/// Uses the C API `duckdb_get_list_size` and `duckdb_get_list_child` to iterate
/// the list, then `duckdb_get_varchar` on each child element.
///
/// # Safety
///
/// `value` must represent a LIST(VARCHAR) value.
pub(crate) unsafe fn extract_list_strings(value: &Value) -> Vec<String> {
    let value_ptr = value_raw_ptr(value);
    let size = ffi::duckdb_get_list_size(value_ptr);
    let mut result = Vec::with_capacity(size as usize);
    for i in 0..size {
        let child = ffi::duckdb_get_list_child(value_ptr, i);
        let cstr = ffi::duckdb_get_varchar(child);
        if !cstr.is_null() {
            let s = CStr::from_ptr(cstr).to_string_lossy().into_owned();
            ffi::duckdb_free(cstr.cast::<c_void>());
            result.push(s);
        }
        ffi::duckdb_destroy_value(&mut { child });
    }
    result
}

// ---------------------------------------------------------------------------
// VTab implementation
// ---------------------------------------------------------------------------

/// The `semantic_query` table function.
///
/// Registered as a single generic function that accepts a view name as a
/// positional parameter and optional `dimensions` / `metrics` as named
/// `LIST(VARCHAR)` parameters.
///
/// Usage:
/// ```sql
/// FROM semantic_query('my_view', dimensions := ['region'], metrics := ['total_revenue'])
/// ```
pub struct SemanticViewVTab;

impl VTab for SemanticViewVTab {
    type BindData = SemanticViewBindData;
    type InitData = SemanticViewInitData;

    fn bind(bind: &BindInfo) -> Result<Self::BindData, Box<dyn std::error::Error>> {
        // 1. Extract the view name (positional parameter 0).
        let view_name = bind.get_parameter(0).to_string();

        // 2. Extract dimensions and metrics from named LIST(VARCHAR) parameters.
        //    Use the raw duckdb_value FFI to iterate list elements.
        let dimensions = match bind.get_named_parameter("dimensions") {
            Some(ref val) => unsafe { extract_list_strings(val) },
            None => vec![],
        };
        let metrics = match bind.get_named_parameter("metrics") {
            Some(ref val) => unsafe { extract_list_strings(val) },
            None => vec![],
        };

        // 3. Validate: at least one dimension or metric.
        if dimensions.is_empty() && metrics.is_empty() {
            return Err(Box::new(QueryError::EmptyRequest {
                view_name: view_name.clone(),
            }));
        }

        // 4. Look up view definition in the catalog.
        let state_ptr = bind.get_extra_info::<QueryState>();
        let state = unsafe { &*state_ptr };
        let catalog_guard = state.catalog.read().expect("catalog RwLock poisoned");

        let json_str = match catalog_guard.get(&view_name) {
            Some(j) => j.clone(),
            None => {
                let available: Vec<String> = catalog_guard.keys().cloned().collect();
                let suggestion = suggest_closest(&view_name, &available);
                return Err(Box::new(QueryError::ViewNotFound {
                    name: view_name,
                    suggestion,
                    available,
                }));
            }
        };
        // Release the catalog lock before proceeding.
        drop(catalog_guard);

        // 5. Parse definition and expand.
        let def = SemanticViewDefinition::from_json(&view_name, &json_str)
            .map_err(|e| -> Box<dyn std::error::Error> { e.into() })?;
        let req = QueryRequest {
            dimensions: dimensions.clone(),
            metrics: metrics.clone(),
        };
        let expanded_sql = expand(&view_name, &def, &req)
            .map_err(|e| -> Box<dyn std::error::Error> { Box::new(QueryError::from(e)) })?;

        // 6. Infer output schema (column names and underlying types for metadata).
        //    Try executing the expanded SQL with LIMIT 0 to discover column types.
        //    If that fails (e.g., re-entrant SQL not allowed in bind), fall back
        //    to defaults: dimensions -> VARCHAR, metrics -> DOUBLE.
        let (column_names, column_type_ids) =
            infer_schema_or_default(state.conn, &expanded_sql, &dimensions, &metrics, &def);

        // 7. Declare all output columns as VARCHAR.
        //    The func() phase reads result data as VARCHAR strings (via a
        //    VARCHAR-cast wrapper query) and writes them using flat_vector::insert.
        //    DuckDB will implicitly cast VARCHAR to target types in downstream
        //    operations (WHERE, ORDER BY, etc.).
        for name in &column_names {
            bind.add_result_column(name, LogicalTypeHandle::from(LogicalTypeId::Varchar));
        }

        Ok(SemanticViewBindData {
            expanded_sql,
            column_names,
            column_type_ids,
        })
    }

    fn init(_: &InitInfo) -> Result<Self::InitData, Box<dyn std::error::Error>> {
        Ok(SemanticViewInitData {
            done: AtomicBool::new(false),
        })
    }

    fn func(
        func: &TableFunctionInfo<Self>,
        output: &mut DataChunkHandle,
    ) -> Result<(), Box<dyn std::error::Error>> {
        let init_data = func.get_init_data();

        // If already done, signal completion.
        if init_data.done.swap(true, Ordering::Relaxed) {
            output.set_len(0);
            return Ok(());
        }

        let bind_data = func.get_bind_data();
        let state = unsafe { &*func.get_extra_info::<QueryState>() };

        // Wrap the expanded SQL so that all columns are cast to VARCHAR.
        // This ensures we only need to read VARCHAR vectors from the result
        // chunks, avoiding type-specific vector parsing. DuckDB will
        // implicitly cast the VARCHAR strings back to the declared output
        // types when they are written via flat_vector::insert.
        let varchar_sql = build_varchar_cast_sql(&bind_data.expanded_sql, &bind_data.column_names);

        // Execute the VARCHAR-wrapped SQL via the raw FFI connection.
        let mut result = unsafe {
            execute_sql_raw(state.conn, &varchar_sql).map_err(|e| QueryError::SqlExecution {
                expanded_sql: bind_data.expanded_sql.clone(),
                duckdb_error: e,
            })?
        };

        // Fetch data from result chunks.
        // All columns are VARCHAR thanks to the wrapping query, so we can
        // read string data directly from the duckdb_string_t layout in
        // each vector.
        let col_count = unsafe { ffi::duckdb_column_count(&mut result) } as usize;
        let chunk_count = unsafe { ffi::duckdb_result_chunk_count(result) } as usize;

        // Collect all rows as string vectors.
        let mut all_rows: Vec<Vec<String>> = Vec::new();

        for chunk_idx in 0..chunk_count {
            let chunk = unsafe { ffi::duckdb_result_get_chunk(result, chunk_idx as ffi::idx_t) };
            if chunk.is_null() {
                continue;
            }
            let row_count = unsafe { ffi::duckdb_data_chunk_get_size(chunk) } as usize;

            for row_idx in 0..row_count {
                let mut row: Vec<String> = Vec::with_capacity(col_count);
                for col_idx in 0..col_count {
                    let s = unsafe { read_varchar_from_vector(chunk, col_idx, row_idx) };
                    row.push(s);
                }
                all_rows.push(row);
            }

            unsafe {
                ffi::duckdb_destroy_data_chunk(&mut { chunk });
            }
        }

        // Write rows to the output DataChunkHandle.
        let n = all_rows.len();
        if n > 0 {
            for col_idx in 0..bind_data.column_names.len().min(col_count) {
                let out_vec = output.flat_vector(col_idx);
                for (row_idx, row) in all_rows.iter().enumerate() {
                    out_vec.insert(row_idx, row[col_idx].as_str());
                }
            }
        }
        output.set_len(n);

        unsafe {
            ffi::duckdb_destroy_result(&mut result);
        }

        Ok(())
    }

    fn parameters() -> Option<Vec<LogicalTypeHandle>> {
        // Positional parameter: view_name (VARCHAR)
        Some(vec![LogicalTypeHandle::from(LogicalTypeId::Varchar)])
    }

    fn named_parameters() -> Option<Vec<(String, LogicalTypeHandle)>> {
        Some(vec![
            (
                "dimensions".to_string(),
                LogicalTypeHandle::list(&LogicalTypeHandle::from(LogicalTypeId::Varchar)),
            ),
            (
                "metrics".to_string(),
                LogicalTypeHandle::list(&LogicalTypeHandle::from(LogicalTypeId::Varchar)),
            ),
        ])
    }
}

// ---------------------------------------------------------------------------
// Schema inference
// ---------------------------------------------------------------------------

/// Try to infer the output schema by executing the expanded SQL with LIMIT 0.
///
/// If schema inference succeeds, returns the actual column names and DuckDB type
/// enums from DuckDB's type inference. If it fails (e.g., re-entrant SQL blocked
/// in bind), falls back to default types: dimension columns as VARCHAR, metric
/// columns as DOUBLE.
fn infer_schema_or_default(
    conn: ffi::duckdb_connection,
    expanded_sql: &str,
    dimensions: &[String],
    metrics: &[String],
    def: &SemanticViewDefinition,
) -> (Vec<String>, Vec<ffi::duckdb_type>) {
    // Try LIMIT 0 for accurate type inference.
    let limit0_sql = format!("{expanded_sql} LIMIT 0");
    let inferred = unsafe { try_infer_schema(conn, &limit0_sql) };

    if let Some((names, types)) = inferred {
        return (names, types);
    }

    // Fallback: derive schema from definition metadata.
    let mut column_names = Vec::new();
    let mut column_types = Vec::new();

    for dim_name in dimensions {
        // Use the definition's canonical name casing.
        let canonical = def
            .dimensions
            .iter()
            .find(|d| d.name.eq_ignore_ascii_case(dim_name))
            .map_or_else(|| dim_name.clone(), |d| d.name.clone());
        column_names.push(canonical);
        column_types.push(ffi::DUCKDB_TYPE_DUCKDB_TYPE_VARCHAR);
    }
    for met_name in metrics {
        let canonical = def
            .metrics
            .iter()
            .find(|m| m.name.eq_ignore_ascii_case(met_name))
            .map_or_else(|| met_name.clone(), |m| m.name.clone());
        column_names.push(canonical);
        column_types.push(ffi::DUCKDB_TYPE_DUCKDB_TYPE_DOUBLE);
    }

    (column_names, column_types)
}

/// Attempt to infer column names and types by executing a LIMIT 0 query.
///
/// Returns `None` if the query fails for any reason.
///
/// # Safety
///
/// `conn` must be a valid `duckdb_connection`.
unsafe fn try_infer_schema(
    conn: ffi::duckdb_connection,
    sql: &str,
) -> Option<(Vec<String>, Vec<ffi::duckdb_type>)> {
    let mut result = execute_sql_raw(conn, sql).ok()?;

    let col_count = ffi::duckdb_column_count(&mut result) as usize;
    let mut names = Vec::with_capacity(col_count);
    let mut types = Vec::with_capacity(col_count);

    for i in 0..col_count {
        let name_ptr = ffi::duckdb_column_name(&mut result, i as ffi::idx_t);
        let name = if name_ptr.is_null() {
            format!("column{i}")
        } else {
            CStr::from_ptr(name_ptr).to_string_lossy().into_owned()
        };
        names.push(name);

        let ty = ffi::duckdb_column_type(&mut result, i as ffi::idx_t);
        types.push(ty);
    }

    ffi::duckdb_destroy_result(&mut result);
    Some((names, types))
}

// ---------------------------------------------------------------------------
// VARCHAR-cast SQL wrapping and chunk-based string reading
// ---------------------------------------------------------------------------

/// Wrap the expanded SQL in a subquery that casts every output column to VARCHAR.
///
/// The deprecated `duckdb_value_varchar` API does not work reliably with DuckDB's
/// chunked result format. By casting all columns to VARCHAR in the SQL itself,
/// we guarantee that every vector in every result chunk contains `duckdb_string_t`
/// values, which can be read uniformly via `read_varchar_from_vector`.
fn build_varchar_cast_sql(expanded_sql: &str, column_names: &[String]) -> String {
    use crate::expand::quote_ident;

    let cast_items: Vec<String> = column_names
        .iter()
        .map(|name| {
            format!(
                "CAST({} AS VARCHAR) AS {}",
                quote_ident(name),
                quote_ident(name)
            )
        })
        .collect();
    format!(
        "SELECT {} FROM ({}) AS \"_sq\"",
        cast_items.join(", "),
        expanded_sql
    )
}

/// Read a VARCHAR value from a data chunk vector at the given column and row.
///
/// The vector must contain VARCHAR (`duckdb_string_t`) data. Returns an empty
/// string for NULL values.
///
/// Decodes the `duckdb_string_t` layout directly from vector memory to avoid
/// reliance on C API helper functions that may not be available in loadable
/// extension mode.
///
/// # Safety
///
/// `chunk` must be a valid, non-null `duckdb_data_chunk` handle.
/// `col_idx` and `row_idx` must be within bounds.
#[allow(clippy::cast_possible_truncation)]
pub(crate) unsafe fn read_varchar_from_vector(
    chunk: ffi::duckdb_data_chunk,
    col_idx: usize,
    row_idx: usize,
) -> String {
    let vector = ffi::duckdb_data_chunk_get_vector(chunk, col_idx as ffi::idx_t);

    // Check for NULL using the validity mask.
    let validity = ffi::duckdb_vector_get_validity(vector);
    if !validity.is_null() {
        let entry_idx = row_idx / 64;
        let bit_idx = row_idx % 64;
        let entry = *validity.add(entry_idx);
        if entry & (1u64 << bit_idx) == 0 {
            return String::new(); // NULL
        }
    }

    // Read the duckdb_string_t from the vector data.
    // Layout is a 16-byte union:
    //   Inline  (len <= 12): { length: u32, inlined: [c_char; 12] }
    //   Pointer (len > 12):  { length: u32, prefix: [c_char; 4], ptr: *mut c_char }
    let data_ptr = ffi::duckdb_vector_get_data(vector);
    let string_t_ptr = data_ptr.cast::<ffi::duckdb_string_t>().add(row_idx);
    let string_t = &*string_t_ptr;

    // The length field is at the same offset for both union variants.
    let len = string_t.value.inlined.length as usize;
    if len == 0 {
        return String::new();
    }

    let bytes = if len <= 12 {
        // Inline: string data follows the length field directly.
        let inline_ptr = string_t.value.inlined.inlined.as_ptr().cast::<u8>();
        std::slice::from_raw_parts(inline_ptr, len)
    } else {
        // Pointer: data is at the heap-allocated ptr.
        let heap_ptr = string_t.value.pointer.ptr.cast::<u8>();
        if heap_ptr.is_null() {
            return String::new();
        }
        std::slice::from_raw_parts(heap_ptr, len)
    };

    String::from_utf8_lossy(bytes).into_owned()
}
